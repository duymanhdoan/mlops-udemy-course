{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classifier_pytorch_101.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBflgWgs-eWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3025d963-9099-40e5-82e3-06266a5f81d5"
      },
      "source": [
        "!pip install torch==1.0.1 torchvision==0.2.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.0.1 in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: torchvision==0.2.2 in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
            "Requirement already satisfied: tqdm==4.19.9 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.2) (4.19.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.2) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.2) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.2) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szSdz2Eh_A0M"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG6vNyHEQoB1"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0KHwOZ0Q6jh",
        "outputId": "4cc29ada-a093-4d22-fd81-bb3b1c4d52a2"
      },
      "source": [
        "nltk.download('all')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0J_Sj_eRAO-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG0MHsxlRm0e"
      },
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/futurexskill/ml-model-deployment/main/Restaurant_Reviews.tsv.txt', delimiter= '\\t', quoting = 3)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "Qj4QUwy7RtEi",
        "outputId": "5a01ccb2-e107-4cc1-cd3e-e577b019bf5f"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review  Liked\n",
              "0                           Wow... Loved this place.      1\n",
              "1                                 Crust is not good.      0\n",
              "2          Not tasty and the texture was just nasty.      0\n",
              "3  Stopped by during the late May bank holiday of...      1\n",
              "4  The selection on the menu was great and so wer...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Lb1G3NSZ0L"
      },
      "source": [
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZogW8b3DTGQ8"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIwfo-1GTKWD"
      },
      "source": [
        "ps = PorterStemmer()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCxuGsgPTM3_",
        "outputId": "a0442cfb-af25-43d8-e701-54e8ee439350"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  1000 non-null   object\n",
            " 1   Liked   1000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MC79MlzTRgY"
      },
      "source": [
        "corpus = []\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUseJMNPVUzH"
      },
      "source": [
        "import re"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH7ziNMCTYiy"
      },
      "source": [
        "for i in range(0, 1000):\n",
        "\n",
        "  customer_review = re.sub('[^a-zA-Z]', ' ',dataset['Review'][i])\n",
        "  customer_review = customer_review.lower()\n",
        "  customer_review = customer_review.split()\n",
        "  clean_review = [ps.stem(word) for word in customer_review if not word in set(stopwords.words('english'))]\n",
        "  clean_review = ' '.join(clean_review)\n",
        "  corpus.append(clean_review)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Twuqh6fsVc6d",
        "outputId": "3fdcba50-4823-408d-a3a1-fc63700890f1"
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'wow love place'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DOr4rGoLVpAc",
        "outputId": "867e076c-7efc-4daf-a83a-ed51dff5bc19"
      },
      "source": [
        "corpus[6]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'honeslti tast fresh'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AmFOCByVVwpm",
        "outputId": "fc1a77fc-fb15-445f-f00d-d4a7992cef37"
      },
      "source": [
        "corpus[12]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cashier care ever say still end wayyy overpr'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGOH342dWDUU"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features = 1500, min_df = 3, max_df = 0.6)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0uX9FMEWN-5"
      },
      "source": [
        "X = vectorizer.fit_transform(corpus).toarray()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VJx99uhW3Uc",
        "outputId": "186e725c-b176-4968-a090-9178a1580573"
      },
      "source": [
        "X"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eN5zp88W4D9",
        "outputId": "92886d2b-4c44-414a-adbb-19d662094a04"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.51611335, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.37891311, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.76814834, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cg_taoYW_gA"
      },
      "source": [
        "y = dataset.iloc[:, 1].values"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Acy_ksXRkz",
        "outputId": "a02dc089-5a0a-4dd8-d249-ea2051549074"
      },
      "source": [
        "y"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFwvHO6vXSQT"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvUHOUvsyS34"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hk-9ln-AhHi",
        "outputId": "575223c5-43d2-4c9f-f75f-3c68cd26dd1b"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_CqS6CVyM6B"
      },
      "source": [
        "Xtrain_ = torch.from_numpy(X_train).float()\n",
        "Xtest_ = torch.from_numpy(X_test).float()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTp5us_HXXEw"
      },
      "source": [
        "ytrain_ = torch.from_numpy(y_train)\n",
        "ytest_ = torch.from_numpy(y_test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HcNA4eDXee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a243cfcd-ff2d-486a-bc79-bda268aa19bc"
      },
      "source": [
        "Xtrain_.shape, ytrain_.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([800, 467]), torch.Size([800]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZfiDEj3XnL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a8ab62c-a25f-4437-fcdd-d77e7cb98bcc"
      },
      "source": [
        "Xtest_.shape, ytest_.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([200, 467]), torch.Size([200]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXVUjwR6XqI9"
      },
      "source": [
        "input_size=467\n",
        "output_size=2\n",
        "hidden_size=500"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZgwPR8tXsiE"
      },
      "source": [
        "class Net(nn.Module):\n",
        "   def __init__(self):\n",
        "       super(Net, self).__init__()\n",
        "       self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
        "       self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "       self.fc3 = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "   def forward(self, X):\n",
        "       X = torch.relu((self.fc1(X)))\n",
        "       X = torch.relu((self.fc2(X)))\n",
        "       X = self.fc3(X)\n",
        "\n",
        "       return F.log_softmax(X,dim=1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c933ZQOyXuQd"
      },
      "source": [
        "model = Net()\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgnra037Xxl7"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.NLLLoss()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCGzLzUHX7L_"
      },
      "source": [
        "epochs = 100\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYgpFHAbYDQf",
        "outputId": "b4847934-2638-403c-a7cd-77292e4b59e6"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  Ypred = model(Xtrain_)\n",
        "  loss = loss_fn(Ypred,  ytrain_)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print('Epoch',epoch, 'loss',loss.item())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 loss 0.6933408379554749\n",
            "Epoch 1 loss 0.6712999939918518\n",
            "Epoch 2 loss 0.5471299886703491\n",
            "Epoch 3 loss 0.35765624046325684\n",
            "Epoch 4 loss 0.2313990592956543\n",
            "Epoch 5 loss 0.16735348105430603\n",
            "Epoch 6 loss 0.10427160561084747\n",
            "Epoch 7 loss 0.09019485116004944\n",
            "Epoch 8 loss 0.07943294942378998\n",
            "Epoch 9 loss 0.06255828589200974\n",
            "Epoch 10 loss 0.045051902532577515\n",
            "Epoch 11 loss 0.044423364102840424\n",
            "Epoch 12 loss 0.04883534461259842\n",
            "Epoch 13 loss 0.039201803505420685\n",
            "Epoch 14 loss 0.03475183621048927\n",
            "Epoch 15 loss 0.03639525547623634\n",
            "Epoch 16 loss 0.03609052672982216\n",
            "Epoch 17 loss 0.03316528722643852\n",
            "Epoch 18 loss 0.03137346729636192\n",
            "Epoch 19 loss 0.030696826055645943\n",
            "Epoch 20 loss 0.03220561146736145\n",
            "Epoch 21 loss 0.03090761974453926\n",
            "Epoch 22 loss 0.029216811060905457\n",
            "Epoch 23 loss 0.029901456087827682\n",
            "Epoch 24 loss 0.030969781801104546\n",
            "Epoch 25 loss 0.0293154064565897\n",
            "Epoch 26 loss 0.02872937172651291\n",
            "Epoch 27 loss 0.02999374456703663\n",
            "Epoch 28 loss 0.029383979737758636\n",
            "Epoch 29 loss 0.028716428205370903\n",
            "Epoch 30 loss 0.029274430125951767\n",
            "Epoch 31 loss 0.028897659853100777\n",
            "Epoch 32 loss 0.028804635629057884\n",
            "Epoch 33 loss 0.02877996489405632\n",
            "Epoch 34 loss 0.028732886537909508\n",
            "Epoch 35 loss 0.028414439409971237\n",
            "Epoch 36 loss 0.028606735169887543\n",
            "Epoch 37 loss 0.028680868446826935\n",
            "Epoch 38 loss 0.02820749580860138\n",
            "Epoch 39 loss 0.02838902547955513\n",
            "Epoch 40 loss 0.0285946074873209\n",
            "Epoch 41 loss 0.028161142021417618\n",
            "Epoch 42 loss 0.028257159516215324\n",
            "Epoch 43 loss 0.028435546904802322\n",
            "Epoch 44 loss 0.02819792740046978\n",
            "Epoch 45 loss 0.02821439690887928\n",
            "Epoch 46 loss 0.028256678953766823\n",
            "Epoch 47 loss 0.028189849108457565\n",
            "Epoch 48 loss 0.02821100316941738\n",
            "Epoch 49 loss 0.028180470690131187\n",
            "Epoch 50 loss 0.02813497558236122\n",
            "Epoch 51 loss 0.02821272425353527\n",
            "Epoch 52 loss 0.028146810829639435\n",
            "Epoch 53 loss 0.02809351496398449\n",
            "Epoch 54 loss 0.028198104351758957\n",
            "Epoch 55 loss 0.02813021093606949\n",
            "Epoch 56 loss 0.028077714145183563\n",
            "Epoch 57 loss 0.02814987488090992\n",
            "Epoch 58 loss 0.0281158909201622\n",
            "Epoch 59 loss 0.0280932504683733\n",
            "Epoch 60 loss 0.02810821309685707\n",
            "Epoch 61 loss 0.028098203241825104\n",
            "Epoch 62 loss 0.02810361236333847\n",
            "Epoch 63 loss 0.02809421345591545\n",
            "Epoch 64 loss 0.028075575828552246\n",
            "Epoch 65 loss 0.02810026705265045\n",
            "Epoch 66 loss 0.028087515383958817\n",
            "Epoch 67 loss 0.02807760052382946\n",
            "Epoch 68 loss 0.028093362227082253\n",
            "Epoch 69 loss 0.028070125728845596\n",
            "Epoch 70 loss 0.028075478971004486\n",
            "Epoch 71 loss 0.02808625064790249\n",
            "Epoch 72 loss 0.028075527399778366\n",
            "Epoch 73 loss 0.028078386560082436\n",
            "Epoch 74 loss 0.02807631716132164\n",
            "Epoch 75 loss 0.028067825362086296\n",
            "Epoch 76 loss 0.028072478249669075\n",
            "Epoch 77 loss 0.028067369014024734\n",
            "Epoch 78 loss 0.028070706874132156\n",
            "Epoch 79 loss 0.028072359040379524\n",
            "Epoch 80 loss 0.028069550171494484\n",
            "Epoch 81 loss 0.028075551614165306\n",
            "Epoch 82 loss 0.028073616325855255\n",
            "Epoch 83 loss 0.02807755023241043\n",
            "Epoch 84 loss 0.028084494173526764\n",
            "Epoch 85 loss 0.02808946929872036\n",
            "Epoch 86 loss 0.02810823731124401\n",
            "Epoch 87 loss 0.028135232627391815\n",
            "Epoch 88 loss 0.028190618380904198\n",
            "Epoch 89 loss 0.028291024267673492\n",
            "Epoch 90 loss 0.02848953567445278\n",
            "Epoch 91 loss 0.028742220252752304\n",
            "Epoch 92 loss 0.0291411392390728\n",
            "Epoch 93 loss 0.029081344604492188\n",
            "Epoch 94 loss 0.028849413618445396\n",
            "Epoch 95 loss 0.02829940989613533\n",
            "Epoch 96 loss 0.028068849816918373\n",
            "Epoch 97 loss 0.02818346954882145\n",
            "Epoch 98 loss 0.028480779379606247\n",
            "Epoch 99 loss 0.028753764927387238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3cSNTmmYEg1"
      },
      "source": [
        "sample = [\"Good batting by England\"]\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3qftBXlYO3E"
      },
      "source": [
        "sample = vectorizer.transform(sample).toarray()\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpgyEuQCYTCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6603d4f6-a131-4f42-89e3-7d8b883d5e8f"
      },
      "source": [
        "sample"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZNMlfQyYYBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0838fe46-eccb-4c42-f8b1-2ea87af2d107"
      },
      "source": [
        "torch.from_numpy(sample).float()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJkJtzwKYaoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b0cd3ab-da3e-4a41-eff6-c8266e6fc67b"
      },
      "source": [
        "sentiment = model(torch.from_numpy(sample).float())\n",
        "sentiment"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.0209, -0.1422]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frLMoG4kYfWX"
      },
      "source": [
        "sample2 = [\"bad performance by India in the match\"]\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcQ25_ne0K4w"
      },
      "source": [
        "sample2 = vectorizer.transform(sample2).toarray()\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIvI5szPYg0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11877de2-9c1f-42d5-cd5a-b28ec02e5556"
      },
      "source": [
        "sentiment2 = model(torch.from_numpy(sample2).float())\n",
        "sentiment2"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.0000, -44.1050]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkZbp_CC0YTg",
        "outputId": "7fffe80a-37f2-4f37-e60b-41ade48410f5"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('fc1.weight',\n",
              "              tensor([[ 0.0940, -0.0413, -0.0656,  ...,  0.0897, -0.0869,  0.1392],\n",
              "                      [-0.0197, -0.0816, -0.1059,  ...,  0.0228,  0.1195, -0.1405],\n",
              "                      [ 0.0237,  0.0054, -0.0162,  ...,  0.0025,  0.0080, -0.0223],\n",
              "                      ...,\n",
              "                      [-0.0354, -0.0016, -0.0252,  ...,  0.0052,  0.0607,  0.0114],\n",
              "                      [-0.0611,  0.0420, -0.1292,  ..., -0.0420, -0.0125,  0.0076],\n",
              "                      [-0.0167, -0.0350, -0.1169,  ...,  0.0250,  0.1192, -0.0812]])),\n",
              "             ('fc1.bias',\n",
              "              tensor([ 1.1780e-02,  1.1869e-02, -7.3322e-03, -1.0564e-01, -1.0328e-01,\n",
              "                      -6.5869e-03,  5.4558e-03, -5.6038e-02, -7.7382e-02, -5.4056e-02,\n",
              "                       2.0714e-02,  3.7538e-02, -5.9266e-02,  2.1252e-02, -1.0462e-03,\n",
              "                       6.7312e-03,  6.5557e-02,  4.6935e-02,  1.3896e-02,  4.8566e-02,\n",
              "                       3.0712e-02,  2.4049e-02, -3.4352e-03, -1.6105e-02, -6.6454e-02,\n",
              "                       2.0078e-02, -7.3193e-02,  3.1839e-02, -3.2655e-02, -1.4102e-02,\n",
              "                       2.0141e-02, -6.8369e-02,  4.6904e-03, -1.2490e-01,  1.9777e-03,\n",
              "                       1.4821e-02, -1.7027e-02, -6.9286e-03, -7.1359e-03, -3.7343e-02,\n",
              "                       1.6780e-03, -8.6837e-02,  1.1316e-02, -2.0154e-02, -2.7649e-03,\n",
              "                      -4.3854e-03,  1.8164e-02,  4.3949e-02,  1.4671e-05,  2.3773e-02,\n",
              "                      -3.5630e-02,  1.0087e-02, -6.3167e-04,  7.7138e-03, -1.7181e-02,\n",
              "                       5.7229e-03, -2.0379e-02,  1.3676e-02, -8.4536e-02, -1.1592e-01,\n",
              "                       2.8349e-02,  9.6266e-03, -9.2266e-02, -8.2656e-02,  3.0568e-03,\n",
              "                       2.8443e-03,  2.0416e-02, -8.4428e-02,  5.4845e-02,  1.4177e-02,\n",
              "                      -2.6278e-02,  2.8770e-02, -1.0798e-01, -7.0690e-02,  9.8997e-04,\n",
              "                      -3.4191e-02, -1.2324e-02,  2.5297e-02,  3.7233e-02,  3.3388e-02,\n",
              "                       9.2741e-03, -1.3594e-01, -1.0214e-02, -4.8215e-02, -2.1627e-02,\n",
              "                      -1.0882e-01, -3.8427e-02,  1.8236e-02,  8.5001e-03,  1.1312e-02,\n",
              "                      -1.5340e-02, -1.2971e-02, -4.9416e-02, -2.7565e-02, -7.6577e-02,\n",
              "                      -1.0675e-02, -6.6945e-02, -9.7525e-03, -2.8128e-02,  3.2798e-02,\n",
              "                      -1.7032e-02, -1.3600e-02,  1.4046e-02,  4.7282e-03,  3.9460e-02,\n",
              "                       5.1004e-02, -3.2477e-02,  1.9169e-02,  1.1588e-02, -1.1592e-01,\n",
              "                      -3.2706e-02, -2.8516e-02,  6.1787e-03,  4.8209e-03, -1.4122e-01,\n",
              "                      -7.2798e-02, -3.3899e-02, -3.2547e-02,  6.4040e-02, -8.8448e-03,\n",
              "                      -7.4305e-02,  2.4753e-02,  2.1000e-02, -4.6944e-03, -3.7394e-03,\n",
              "                      -9.7172e-02, -3.6782e-02, -8.0705e-03, -2.4153e-02, -7.9086e-03,\n",
              "                      -3.3575e-02, -9.0286e-02, -2.4791e-02,  1.2326e-02,  5.1123e-02,\n",
              "                      -1.6195e-02, -9.4861e-02,  2.6179e-02, -1.0275e-01, -9.3888e-02,\n",
              "                      -2.7461e-02,  9.6499e-03, -5.7586e-02, -5.0124e-02, -4.8433e-02,\n",
              "                      -2.0479e-02, -4.9429e-02, -3.4555e-02,  3.0544e-02, -3.8154e-02,\n",
              "                       6.4289e-03, -2.5835e-03,  4.3057e-02, -5.5165e-02,  2.9470e-02,\n",
              "                       1.2918e-02, -3.9213e-03, -6.2977e-03,  1.7192e-02,  3.4079e-02,\n",
              "                      -5.6248e-02,  3.2994e-02, -4.7128e-02,  5.4746e-02,  4.9799e-02,\n",
              "                      -7.7166e-02, -1.8821e-02, -8.7022e-02, -2.0979e-02,  1.1396e-02,\n",
              "                      -1.3443e-02, -1.0340e-01,  7.0138e-03,  1.2532e-02, -1.7568e-02,\n",
              "                       1.0199e-02, -1.2265e-01,  9.2609e-03, -1.9089e-02, -4.0908e-03,\n",
              "                       7.6008e-03, -9.5838e-02,  2.8162e-02, -3.8929e-02, -1.1229e-02,\n",
              "                       4.9826e-02,  5.3247e-02,  5.5602e-02,  1.9542e-02, -7.6340e-02,\n",
              "                      -5.7157e-02, -7.3368e-03, -1.3741e-02, -2.3843e-02, -9.9102e-03,\n",
              "                      -9.4716e-02, -5.2881e-02,  1.1557e-02, -5.9255e-02, -1.9162e-02,\n",
              "                      -8.8712e-03, -1.3528e-02,  1.7769e-02, -4.2306e-02,  2.1176e-02,\n",
              "                      -1.8522e-02,  1.2842e-02,  5.0793e-02,  2.3338e-02, -3.5358e-02,\n",
              "                       6.0222e-02, -2.6033e-02, -6.9151e-02,  1.7638e-02, -2.8029e-02,\n",
              "                      -2.7962e-02,  1.2562e-02,  3.4428e-02,  4.1232e-02, -2.3656e-02,\n",
              "                      -1.9677e-02,  4.0713e-02,  5.6461e-02, -1.1743e-02,  3.9367e-02,\n",
              "                      -3.8583e-02,  1.5413e-02,  1.1723e-02, -3.3711e-02,  3.0433e-03,\n",
              "                      -1.7003e-02,  4.3924e-03,  5.9966e-02,  1.2653e-02, -1.2930e-02,\n",
              "                       3.2413e-02,  2.5942e-02,  8.2104e-03,  3.3975e-02,  1.7526e-02,\n",
              "                      -3.1653e-02, -1.3766e-02, -3.8467e-02,  2.5373e-02,  3.8699e-02,\n",
              "                      -8.8442e-02, -7.5246e-02, -7.5000e-02,  2.9079e-02, -2.7983e-02,\n",
              "                      -4.2882e-02, -9.8484e-02, -4.9450e-02,  2.8520e-02, -3.1705e-02,\n",
              "                      -2.1007e-02,  4.5226e-02, -1.7183e-03, -3.9953e-02,  1.0787e-02,\n",
              "                       1.7314e-02, -4.5975e-02, -1.1474e-01,  3.9945e-02, -6.3630e-02,\n",
              "                      -3.8828e-02,  3.3545e-02, -2.0355e-02, -7.2431e-02, -3.9080e-02,\n",
              "                      -2.7777e-02,  4.0516e-02,  1.7099e-02,  9.1683e-03,  2.7690e-02,\n",
              "                      -5.9747e-03,  4.0167e-02,  2.2175e-02, -1.9585e-02,  3.0261e-02,\n",
              "                       3.4083e-02, -4.4975e-02,  1.8084e-02,  5.0841e-02, -1.4209e-02,\n",
              "                       6.4398e-03, -7.5596e-02, -2.2041e-02,  2.4308e-02, -2.7945e-02,\n",
              "                      -2.7678e-02, -1.3294e-02, -1.1526e-02, -6.5732e-02,  4.9960e-02,\n",
              "                      -1.2360e-01,  4.0743e-02, -1.0058e-02,  2.5171e-02, -9.8981e-03,\n",
              "                       5.3558e-02, -1.0869e-01, -4.0584e-02, -1.5446e-02, -5.5195e-02,\n",
              "                       4.0174e-02,  4.0991e-02, -2.6053e-02, -3.8885e-02, -2.2556e-02,\n",
              "                       1.0125e-02, -4.0525e-03, -9.5080e-03,  6.5828e-03, -4.3561e-02,\n",
              "                      -8.2902e-03, -1.0881e-01, -9.5539e-02, -3.0181e-02, -7.8205e-02,\n",
              "                       2.0265e-02,  1.1672e-02,  2.2741e-02, -7.3413e-02, -4.2870e-02,\n",
              "                      -2.0456e-02,  3.0485e-03, -5.7464e-02, -2.9051e-03, -1.7907e-02,\n",
              "                      -7.7136e-02,  2.3347e-02, -1.5149e-02, -1.1732e-01, -3.2159e-02,\n",
              "                       1.2317e-02, -7.5357e-02, -7.8945e-03, -6.1609e-02, -4.8268e-02,\n",
              "                      -1.0193e-01,  5.4793e-03, -2.3004e-04, -3.1543e-02, -3.6503e-02,\n",
              "                      -5.0862e-02,  1.1061e-02, -1.0286e-01,  2.0889e-02, -1.9045e-02,\n",
              "                       1.9477e-02, -2.5377e-02,  2.5121e-03,  2.2508e-02, -1.6081e-02,\n",
              "                      -7.0600e-02, -6.5795e-02, -1.0327e-01, -1.1809e-02, -5.3188e-02,\n",
              "                      -9.9415e-02, -5.9414e-02, -2.0868e-03,  1.9657e-02, -6.5473e-02,\n",
              "                      -7.3939e-02, -2.3763e-02, -6.4874e-02, -1.7293e-02,  3.2060e-02,\n",
              "                      -7.3167e-03, -2.0133e-02,  4.4903e-02,  5.5347e-02,  7.9513e-03,\n",
              "                       3.1407e-04, -8.9082e-02, -9.1569e-02,  6.8402e-03, -3.2760e-03,\n",
              "                      -2.2613e-02,  4.4389e-02, -2.9832e-02, -7.6015e-02, -3.5806e-02,\n",
              "                      -1.8753e-02, -7.7677e-02,  2.6490e-03, -2.1489e-02,  1.2540e-02,\n",
              "                      -3.4221e-02, -7.6275e-02, -1.1168e-01, -1.8228e-02,  1.9884e-02,\n",
              "                       2.7402e-02,  1.2975e-02,  6.2243e-03, -1.5857e-02, -2.0491e-02,\n",
              "                       4.2445e-02,  1.4564e-02,  2.1594e-02, -5.8109e-02,  4.7306e-02,\n",
              "                       4.0786e-02, -2.8402e-02,  1.4360e-02, -1.5980e-02, -9.6858e-03,\n",
              "                      -4.7601e-02, -1.1702e-01, -4.6794e-03,  1.4375e-02, -9.6310e-02,\n",
              "                       2.7512e-02,  1.2577e-02, -8.4739e-02,  3.0734e-02,  4.0097e-02,\n",
              "                       2.5215e-02,  3.7571e-02,  4.0396e-02, -8.2484e-02, -1.5379e-02,\n",
              "                      -5.8270e-02, -1.7005e-02, -1.9780e-02,  1.0097e-03, -1.6701e-02,\n",
              "                      -1.1696e-01, -3.6005e-03,  1.1265e-03,  1.2315e-02,  3.9004e-02,\n",
              "                       3.0207e-02,  1.6946e-02,  2.6396e-02,  4.0923e-02, -5.4836e-02,\n",
              "                       1.9825e-02, -8.4001e-02, -4.4320e-02,  3.9041e-02, -7.9185e-02,\n",
              "                      -7.2469e-02,  3.9817e-02, -8.8793e-03, -2.9970e-02,  4.1033e-02,\n",
              "                      -8.9162e-03, -1.3806e-02, -3.0712e-02, -7.8725e-02,  3.7047e-02,\n",
              "                      -2.3917e-03, -2.7951e-02, -1.1857e-02,  1.8572e-02, -1.0833e-01,\n",
              "                      -1.6992e-02,  1.3004e-02,  5.0030e-02,  4.2704e-02,  1.0685e-02,\n",
              "                      -9.6722e-02,  3.4306e-02,  2.6157e-03,  5.2002e-03, -1.8889e-03,\n",
              "                      -1.3982e-02, -1.0101e-01,  6.7600e-02, -7.5612e-02, -1.1599e-01,\n",
              "                      -3.8936e-02,  1.6885e-02, -1.6452e-02, -2.6937e-02,  1.7249e-02,\n",
              "                      -2.9909e-02, -8.3349e-03,  5.5489e-03, -2.9494e-02,  1.7161e-02,\n",
              "                      -1.9461e-02, -4.4100e-03,  4.1096e-02,  7.3445e-03, -1.3281e-02,\n",
              "                       2.7991e-02,  2.0498e-02,  8.8796e-03,  1.5697e-02, -6.4214e-02,\n",
              "                      -7.9650e-02, -3.2498e-03, -9.5110e-02, -1.8784e-02,  8.1536e-03])),\n",
              "             ('fc2.weight',\n",
              "              tensor([[-0.0756, -0.0342, -0.0226,  ..., -0.0103, -0.0534, -0.0046],\n",
              "                      [-0.1243,  0.0093, -0.0243,  ...,  0.1399, -0.0464,  0.0457],\n",
              "                      [ 0.1249, -0.0293, -0.0631,  ...,  0.0291, -0.0063, -0.0051],\n",
              "                      ...,\n",
              "                      [-0.1334,  0.1227,  0.0790,  ...,  0.1106,  0.0893,  0.1199],\n",
              "                      [ 0.1061, -0.0648, -0.0771,  ..., -0.0760, -0.0289,  0.0443],\n",
              "                      [-0.0803,  0.1458,  0.0669,  ...,  0.1188,  0.0409,  0.0591]])),\n",
              "             ('fc2.bias',\n",
              "              tensor([-0.0679,  0.0393,  0.0513, -0.0021,  0.0739,  0.0849, -0.0606,  0.0575,\n",
              "                       0.0236,  0.0612, -0.0633,  0.0523, -0.0689,  0.0597, -0.0357,  0.0535,\n",
              "                      -0.0331,  0.0702,  0.0523,  0.0238,  0.0106,  0.0811,  0.0213, -0.0293,\n",
              "                       0.0475,  0.0637,  0.0427, -0.0751, -0.0472,  0.0417, -0.0399,  0.0283,\n",
              "                      -0.0792,  0.0214, -0.0727,  0.0630,  0.0435, -0.0710,  0.0380,  0.0478,\n",
              "                      -0.0809,  0.0622,  0.0365,  0.0371,  0.0087, -0.0178, -0.0951, -0.0524,\n",
              "                       0.0441,  0.0371, -0.0025,  0.0619, -0.0611, -0.0425, -0.0742,  0.0578,\n",
              "                       0.0571,  0.0603, -0.0246,  0.0208, -0.1056, -0.0029, -0.0943, -0.0140,\n",
              "                       0.0567, -0.0237, -0.1030,  0.0182,  0.0498,  0.0586,  0.0696,  0.0180,\n",
              "                       0.0273, -0.0831, -0.0800,  0.0657, -0.0408, -0.0513, -0.0830, -0.0096,\n",
              "                       0.0749,  0.0326,  0.0389, -0.0803,  0.0654, -0.0579, -0.0783,  0.0651,\n",
              "                      -0.0942, -0.0528, -0.0609, -0.0598,  0.0840, -0.0282, -0.0019,  0.0364,\n",
              "                       0.0732,  0.0365, -0.0643, -0.0207,  0.0709, -0.0900,  0.0427, -0.0094,\n",
              "                      -0.0297, -0.0271, -0.0919,  0.0715, -0.0392, -0.0525, -0.0414,  0.0304,\n",
              "                      -0.0675, -0.0274,  0.0538, -0.0559,  0.0502,  0.0040,  0.0672,  0.0568,\n",
              "                       0.0566,  0.0176,  0.0619, -0.0066, -0.0149,  0.0303, -0.0010, -0.0444,\n",
              "                      -0.0343,  0.0463, -0.0267,  0.0170,  0.0530, -0.0582, -0.0414, -0.0090,\n",
              "                      -0.0390, -0.0491,  0.0447, -0.0347,  0.0747, -0.0070,  0.0784,  0.0415,\n",
              "                       0.0615,  0.0828,  0.0507,  0.0328,  0.0391,  0.0422,  0.0659,  0.0643,\n",
              "                      -0.0918, -0.0069, -0.0028, -0.0416,  0.0607,  0.0148,  0.0624,  0.0107,\n",
              "                       0.0543,  0.0641, -0.0859, -0.0808,  0.0314,  0.0588, -0.0727,  0.0607,\n",
              "                       0.0269,  0.0210,  0.0662,  0.0610, -0.0249, -0.0573,  0.0111,  0.0485,\n",
              "                       0.0805, -0.0217, -0.0917, -0.0278,  0.0429,  0.0190,  0.0436, -0.0790,\n",
              "                      -0.0773,  0.0404,  0.0669, -0.0435,  0.0836, -0.0920, -0.0251, -0.0434,\n",
              "                       0.0848,  0.0272,  0.0693,  0.0397, -0.0138,  0.0593,  0.0532, -0.0458,\n",
              "                       0.0609, -0.0464,  0.0634,  0.0243,  0.0284,  0.0697,  0.0109,  0.0676,\n",
              "                       0.0190, -0.0635,  0.0289,  0.0504, -0.0499,  0.0552, -0.0425,  0.0454,\n",
              "                      -0.0425,  0.0496,  0.0491,  0.0373,  0.0572, -0.0887,  0.0583,  0.0802,\n",
              "                      -0.0932,  0.0644,  0.0087,  0.0587,  0.0643,  0.0291, -0.0844, -0.0738,\n",
              "                      -0.0381,  0.0481, -0.0315, -0.0121,  0.0723,  0.0445,  0.0209,  0.0492,\n",
              "                      -0.0111,  0.0833, -0.0648, -0.0076,  0.0836, -0.0887,  0.0617, -0.0463,\n",
              "                       0.0618, -0.0901,  0.0073, -0.0361,  0.0619, -0.0805,  0.0256,  0.0506,\n",
              "                       0.0643, -0.0497,  0.0807,  0.0530,  0.0151, -0.0232,  0.0736,  0.0549,\n",
              "                       0.0580,  0.0040,  0.0444, -0.0750,  0.0359, -0.0246, -0.0626, -0.0257,\n",
              "                       0.0498, -0.0214, -0.0967,  0.0323,  0.0214, -0.0694,  0.0265, -0.0576,\n",
              "                       0.0335, -0.0034, -0.0894, -0.0672, -0.0676,  0.0270, -0.0318,  0.0308,\n",
              "                      -0.0762, -0.0750, -0.0726, -0.0585,  0.0786, -0.0670, -0.0407,  0.0454,\n",
              "                       0.0110, -0.0348, -0.0379,  0.0059,  0.0207,  0.0607, -0.0737,  0.0029,\n",
              "                      -0.0447, -0.0398,  0.0344, -0.0919,  0.0764, -0.0676, -0.0583,  0.0476,\n",
              "                      -0.0870,  0.0502, -0.0819, -0.0655, -0.0026, -0.0743, -0.0434, -0.0701,\n",
              "                       0.0655, -0.0901, -0.0160, -0.0444,  0.0382,  0.0311,  0.0730, -0.0817,\n",
              "                      -0.0240,  0.0641, -0.0850,  0.0629,  0.0636,  0.0299,  0.0726,  0.0723,\n",
              "                       0.0094,  0.0566,  0.0354, -0.0501,  0.0249,  0.0475, -0.0475,  0.0768,\n",
              "                       0.0406,  0.0416,  0.0367,  0.0284,  0.0407,  0.0393,  0.0322,  0.0227,\n",
              "                      -0.0518, -0.0522,  0.0431, -0.0859, -0.0093,  0.0716,  0.0465,  0.0234,\n",
              "                      -0.0120, -0.0354,  0.0492,  0.0625, -0.0361, -0.0193,  0.0156,  0.0311,\n",
              "                      -0.0681,  0.0548,  0.0576,  0.0588, -0.0902, -0.0476,  0.0327,  0.0132,\n",
              "                      -0.0849, -0.0722, -0.0321,  0.0164,  0.0423,  0.0216, -0.0582,  0.0585,\n",
              "                       0.0166,  0.0387,  0.0570,  0.0595,  0.0457,  0.0748,  0.0440,  0.0282,\n",
              "                       0.0511,  0.0244,  0.0662, -0.0857,  0.0307,  0.0376, -0.0441, -0.0356,\n",
              "                       0.0398, -0.0532,  0.0442, -0.0814,  0.0742,  0.0611,  0.0460, -0.0906,\n",
              "                      -0.0357, -0.0576,  0.0129, -0.0451,  0.0681,  0.0110,  0.0528, -0.0597,\n",
              "                       0.0824, -0.0862,  0.0369,  0.0555,  0.0228,  0.0294,  0.0295, -0.0264,\n",
              "                       0.0672,  0.0093, -0.0010,  0.0506,  0.0709,  0.0582, -0.0985, -0.0681,\n",
              "                       0.0566, -0.0040, -0.0583,  0.0707,  0.0655, -0.0770,  0.0206,  0.0340,\n",
              "                      -0.0396,  0.0230,  0.0760,  0.0473, -0.0513,  0.0561,  0.0191,  0.0451,\n",
              "                       0.0447,  0.0582, -0.0194,  0.0299, -0.0593, -0.1109, -0.0758,  0.0747,\n",
              "                       0.0090,  0.0249, -0.0412,  0.0554, -0.0864, -0.0163,  0.0694,  0.0755,\n",
              "                       0.0583,  0.0402,  0.0585, -0.0475, -0.0790, -0.0447, -0.0593,  0.0555,\n",
              "                       0.0195, -0.0058, -0.0831, -0.0350,  0.0170, -0.0730,  0.0642, -0.0031,\n",
              "                       0.0204,  0.0888, -0.0773, -0.0318,  0.0467,  0.0652,  0.0327, -0.0639,\n",
              "                       0.0613,  0.0599, -0.0185, -0.0206, -0.0791,  0.0589, -0.0440, -0.0326,\n",
              "                      -0.0297,  0.0648,  0.0295,  0.0311])),\n",
              "             ('fc3.weight',\n",
              "              tensor([[ 3.4808e-02, -1.3897e-02,  1.0444e-01, -7.8588e-02,  1.2867e-01,\n",
              "                       -1.1411e-01,  6.3992e-02,  8.4010e-02,  1.5654e-02, -8.3884e-02,\n",
              "                       -2.9395e-02, -8.2135e-02, -1.9011e-02, -8.3320e-02,  2.9630e-02,\n",
              "                        1.0230e-01,  8.2902e-02, -9.0209e-02,  6.4912e-02, -9.9479e-02,\n",
              "                        8.0612e-02,  9.8305e-02, -1.1315e-01,  2.0733e-02, -5.9814e-02,\n",
              "                       -7.7970e-02, -1.2617e-01, -1.9342e-02, -1.2133e-03, -7.5849e-02,\n",
              "                       -5.2431e-02, -6.9369e-02, -1.0207e-02, -1.2848e-01, -3.9174e-03,\n",
              "                       -8.5368e-02,  1.0131e-01, -6.7582e-02,  4.7161e-02, -6.2087e-02,\n",
              "                        5.1554e-02, -1.2546e-01,  1.2444e-01, -9.4974e-02,  7.2047e-02,\n",
              "                       -1.0787e-02,  7.8102e-03, -3.3998e-02, -1.0988e-01,  7.1384e-02,\n",
              "                        9.4622e-02, -1.0042e-01, -1.7811e-02,  1.5253e-03, -3.8081e-02,\n",
              "                        9.6336e-02, -7.6095e-02,  7.8589e-02, -4.4906e-02, -1.0023e-01,\n",
              "                       -2.9971e-02,  4.6980e-02, -1.2220e-02,  2.4790e-03, -1.1281e-01,\n",
              "                        7.6008e-02,  2.8298e-03, -1.1773e-01, -6.5919e-02,  1.1159e-01,\n",
              "                        1.1733e-01,  5.4366e-02, -6.3052e-02, -8.0784e-02,  4.4730e-03,\n",
              "                       -9.4141e-02,  3.1532e-02,  2.1938e-02, -4.5600e-02,  7.2668e-02,\n",
              "                       -7.9716e-02, -7.1879e-02, -1.0084e-01,  6.3014e-02,  9.8386e-02,\n",
              "                        4.6613e-02, -4.8428e-02, -1.2040e-01, -6.5969e-02,  2.0145e-02,\n",
              "                        1.4199e-02, -2.7032e-02,  1.2519e-01,  1.6132e-02,  9.8408e-02,\n",
              "                       -4.8756e-02, -1.1977e-01, -9.1863e-02, -1.1036e-02, -6.4397e-02,\n",
              "                        7.5239e-02,  6.5984e-02, -8.7137e-02, -8.4467e-02, -1.5643e-02,\n",
              "                       -8.9560e-05,  2.8186e-02,  1.0985e-01, -5.8485e-03,  7.7030e-02,\n",
              "                       -4.0637e-02, -5.9625e-02,  2.7706e-02, -5.7362e-02,  1.1139e-01,\n",
              "                       -8.2679e-02, -7.1255e-02,  8.4713e-02, -6.1097e-02,  8.2154e-02,\n",
              "                       -4.9737e-02,  6.8026e-02, -1.1384e-01, -1.1891e-01,  7.6335e-02,\n",
              "                        3.8698e-02, -5.1562e-02,  6.7020e-02, -3.4774e-02,  6.0284e-02,\n",
              "                       -3.4715e-02, -1.0930e-01,  1.3208e-01, -3.4859e-02, -3.7482e-02,\n",
              "                        1.7961e-02, -5.3933e-04,  1.7183e-02, -8.3054e-02, -2.1214e-02,\n",
              "                       -1.0332e-01,  5.5819e-02, -6.0486e-02,  1.4760e-02, -9.7728e-02,\n",
              "                        1.0778e-01, -3.5409e-02,  4.5504e-02, -7.5792e-02,  7.4901e-02,\n",
              "                        1.1631e-01,  4.9761e-02, -6.3303e-02,  4.1359e-02,  5.2183e-02,\n",
              "                        1.8707e-02, -1.0205e-01, -8.6117e-02, -1.0953e-01,  9.6762e-02,\n",
              "                        1.1347e-01,  1.0919e-01,  3.3436e-02, -4.1753e-02,  1.0562e-02,\n",
              "                        6.6889e-02,  5.6532e-02, -7.6311e-02,  1.0810e-01, -7.3229e-02,\n",
              "                       -9.0967e-02,  5.2243e-02, -3.8914e-02, -6.8436e-04,  3.1335e-02,\n",
              "                       -7.0671e-02,  1.0983e-01,  7.2842e-02,  1.9503e-02, -8.3901e-02,\n",
              "                        5.2659e-02,  2.1469e-02, -5.3471e-02, -2.2030e-02, -5.6600e-02,\n",
              "                        3.3250e-02, -5.3510e-02, -1.9172e-02,  6.6096e-02, -4.0749e-02,\n",
              "                        5.1033e-02, -3.4883e-02, -1.4179e-01, -7.8827e-02,  6.5002e-02,\n",
              "                       -6.0802e-02,  3.6749e-02,  8.4487e-02, -7.6721e-02, -3.3502e-02,\n",
              "                        6.3992e-02, -8.8818e-03, -9.9551e-02, -1.2778e-01, -1.1352e-01,\n",
              "                       -1.1058e-01,  5.6890e-02, -1.1434e-01, -5.2380e-02, -5.6373e-02,\n",
              "                        7.3584e-02, -6.5244e-02, -1.8109e-02, -1.0550e-01, -5.3137e-02,\n",
              "                        8.5686e-02, -4.1859e-02,  9.2550e-02,  5.9681e-02, -5.4354e-02,\n",
              "                        1.0647e-01, -3.8051e-02,  1.0321e-01,  1.1103e-01, -2.4589e-02,\n",
              "                       -8.7629e-02, -7.7500e-02, -1.0398e-01, -8.5968e-02, -5.1307e-02,\n",
              "                        2.4841e-02,  4.5659e-02, -4.1836e-03,  8.0993e-02,  2.3771e-03,\n",
              "                       -1.7397e-02,  7.3430e-02,  6.9408e-02, -6.7642e-02, -1.0871e-01,\n",
              "                        7.3220e-02,  8.5292e-02,  2.3784e-02, -2.8407e-02, -1.0048e-01,\n",
              "                       -5.5681e-02, -5.5838e-02,  3.9289e-02,  6.4514e-02, -4.8675e-03,\n",
              "                        5.6123e-02, -4.1315e-02,  1.0648e-01,  1.4791e-02,  8.7782e-02,\n",
              "                        7.9106e-02, -1.2282e-01, -4.5093e-02,  1.2620e-01, -7.5702e-02,\n",
              "                       -1.2522e-01, -2.0525e-02,  9.5717e-02, -2.7878e-02, -7.4248e-02,\n",
              "                        2.7543e-02,  9.0728e-02, -4.8445e-02, -6.7473e-02,  1.5126e-03,\n",
              "                        3.7795e-02,  7.8922e-02,  9.5084e-02,  1.2270e-03, -1.0690e-02,\n",
              "                        3.5100e-02, -1.2866e-01, -1.5807e-02, -8.6129e-02,  1.5788e-02,\n",
              "                       -7.3127e-02, -6.5032e-02, -2.5822e-02, -3.3766e-02, -1.9289e-02,\n",
              "                       -1.2426e-01,  2.7535e-02, -2.7453e-02, -4.1665e-02,  1.4067e-02,\n",
              "                        2.4829e-03,  3.2871e-03, -1.1750e-01,  3.6498e-02,  4.1465e-02,\n",
              "                       -1.0204e-01, -9.5132e-02, -1.9041e-03,  2.6035e-02, -1.2260e-01,\n",
              "                       -9.6424e-02, -8.2824e-02, -6.0261e-02,  6.4027e-02, -3.0155e-02,\n",
              "                        1.1040e-03, -1.2324e-01, -9.4493e-02,  9.3839e-02, -6.6964e-02,\n",
              "                       -4.0283e-02,  1.0697e-01,  6.5019e-02,  7.0875e-02, -5.9136e-03,\n",
              "                        2.9869e-02,  2.0539e-02,  4.6568e-02,  4.1216e-02, -4.3782e-02,\n",
              "                        9.6379e-02,  4.0237e-02, -6.5554e-02,  7.9112e-03,  9.2216e-02,\n",
              "                        6.4759e-02,  1.3483e-01, -1.3930e-02,  2.5700e-02,  1.1769e-01,\n",
              "                        5.9486e-02,  1.0637e-01, -9.7053e-02,  1.2000e-01, -1.1823e-01,\n",
              "                        1.0461e-01,  4.8537e-02,  9.2937e-02, -1.2202e-01, -6.5276e-02,\n",
              "                        8.4159e-02, -6.6327e-02, -3.7101e-02, -1.0127e-01, -7.3395e-02,\n",
              "                       -7.8566e-02, -6.2682e-02,  3.6171e-02, -1.2076e-01, -5.4687e-02,\n",
              "                       -1.2071e-01,  3.6457e-02,  4.5103e-02, -2.4996e-02, -1.0584e-01,\n",
              "                        1.7093e-02,  9.3140e-03,  1.3180e-01,  1.0067e-01, -8.7963e-02,\n",
              "                       -3.6832e-02,  3.3861e-02, -7.6463e-02,  9.1949e-02,  3.8373e-02,\n",
              "                        6.1863e-04,  1.0199e-01,  2.5288e-02, -6.1613e-02,  3.7396e-02,\n",
              "                       -1.2829e-01, -8.1094e-02,  3.2412e-02, -2.7215e-02,  7.2470e-02,\n",
              "                       -1.1406e-01, -2.7976e-02,  5.1548e-02,  2.4283e-02, -8.9926e-02,\n",
              "                       -8.9029e-02, -2.7421e-02, -3.4280e-02,  8.5673e-02, -1.1725e-01,\n",
              "                       -1.0330e-01, -1.0692e-01, -1.1446e-01, -1.3451e-01,  1.1162e-01,\n",
              "                       -6.1676e-02,  7.0501e-02, -4.0104e-02, -9.2558e-02, -1.1183e-01,\n",
              "                        4.4858e-02,  5.0994e-02,  4.3148e-02, -2.3056e-02, -7.5904e-02,\n",
              "                        8.8751e-02, -1.5990e-02, -4.9183e-02,  5.7717e-02,  1.0173e-01,\n",
              "                       -7.2106e-02,  1.1498e-01, -1.1092e-02, -5.0062e-02, -5.6283e-02,\n",
              "                        1.9470e-02,  5.7574e-02,  8.2833e-02, -2.6513e-02, -1.0677e-01,\n",
              "                       -5.9418e-02, -1.1795e-01, -5.4361e-02,  7.5143e-02,  8.4581e-02,\n",
              "                        6.9892e-02, -1.1037e-01, -2.1691e-02, -2.7729e-03, -5.0966e-02,\n",
              "                       -5.6315e-02, -8.3388e-02,  5.2027e-02, -6.1093e-02,  7.2212e-02,\n",
              "                       -6.1671e-02,  2.8189e-02,  1.0805e-01, -3.9431e-02, -4.8801e-02,\n",
              "                        1.1853e-01, -1.1788e-01, -8.2286e-02, -1.9347e-02, -1.1864e-01,\n",
              "                       -2.0125e-02, -9.2918e-02,  9.5363e-02,  6.2130e-02,  5.0863e-03,\n",
              "                        7.5692e-02,  8.4487e-02, -1.2032e-01,  1.2232e-01,  8.6843e-02,\n",
              "                       -1.1378e-04, -8.2548e-02, -4.6134e-02, -4.0656e-02,  3.9153e-03,\n",
              "                        1.2632e-01,  1.0059e-01, -1.1784e-01, -7.8842e-03,  9.3712e-02,\n",
              "                        3.8408e-02, -7.6025e-02, -8.2185e-02,  9.1592e-02, -1.3389e-01,\n",
              "                        1.0987e-01, -1.0262e-01,  3.0423e-02, -5.0180e-02, -3.0785e-02,\n",
              "                       -6.8663e-02, -7.9021e-02, -6.5272e-02,  6.0165e-02,  2.8507e-02,\n",
              "                       -1.3437e-02,  9.8757e-02, -4.4008e-03,  8.4950e-02,  4.2281e-02,\n",
              "                       -7.0356e-02,  1.2921e-01, -8.5506e-02,  2.2773e-02, -1.0693e-01,\n",
              "                       -1.0137e-01,  2.6616e-02,  1.0168e-01, -9.5747e-02, -1.0963e-01,\n",
              "                       -7.8403e-02,  8.7228e-02, -4.5539e-02, -1.0553e-01,  5.9633e-02,\n",
              "                        9.1506e-02,  9.5003e-03, -9.6674e-02,  5.1069e-02, -7.9086e-02],\n",
              "                      [-2.6887e-02,  8.9535e-02, -1.1961e-01,  5.5484e-02, -1.3251e-01,\n",
              "                        1.2073e-01, -1.6623e-02, -1.0914e-01, -8.7165e-02,  9.2138e-02,\n",
              "                        2.4812e-02,  7.9895e-02,  8.1133e-02,  9.1112e-02,  2.3102e-02,\n",
              "                       -1.0406e-01, -4.0740e-02,  8.4143e-02, -4.2583e-02,  9.3760e-02,\n",
              "                       -9.5220e-02, -8.4217e-02,  3.8784e-02,  2.1581e-02,  7.0125e-02,\n",
              "                        1.1554e-01,  8.1650e-02,  1.3302e-02, -2.4188e-02,  7.9275e-02,\n",
              "                        3.1099e-02,  1.1514e-01, -6.1173e-02,  9.1058e-02,  3.1812e-02,\n",
              "                        6.7711e-02, -8.7347e-02,  2.1464e-02, -8.4421e-02,  3.4077e-02,\n",
              "                       -5.2307e-02,  1.1036e-01, -1.0483e-01,  9.3566e-02, -5.9312e-02,\n",
              "                       -2.1986e-02, -1.6473e-02,  3.9309e-02,  9.2729e-02, -6.0854e-02,\n",
              "                       -7.1866e-02,  8.9037e-02, -3.3483e-02,  1.8151e-02,  6.2253e-02,\n",
              "                       -5.7423e-02,  6.7718e-02, -9.9706e-02,  6.6265e-02,  4.4095e-02,\n",
              "                       -2.6820e-02, -5.9598e-02,  1.4069e-02, -2.1664e-02,  6.4843e-02,\n",
              "                       -8.7425e-02, -1.7713e-02,  1.2593e-01,  9.3941e-03, -7.1562e-02,\n",
              "                       -1.1390e-01, -7.2718e-02,  1.0482e-01,  6.0405e-02,  1.0267e-02,\n",
              "                        1.0330e-01,  4.0922e-02,  2.6731e-02, -9.9115e-03, -8.1897e-02,\n",
              "                        8.8361e-02,  1.0530e-01,  1.1369e-01, -3.2895e-02, -9.3130e-02,\n",
              "                       -3.6976e-02,  1.8164e-02,  1.1814e-01, -1.9910e-03, -3.5709e-02,\n",
              "                       -9.1467e-02,  4.2340e-02, -1.0502e-01,  4.9548e-02, -1.0552e-01,\n",
              "                        1.0957e-01,  9.1931e-02,  7.1235e-02, -2.3661e-02,  5.7816e-02,\n",
              "                       -1.0679e-01, -3.8617e-02,  1.0015e-01,  7.2411e-02,  9.1168e-02,\n",
              "                       -3.2582e-02, -2.5299e-02, -9.3437e-02, -4.9602e-03, -6.9473e-02,\n",
              "                        5.0744e-02,  1.3100e-01, -2.9285e-02,  3.1364e-02, -4.8352e-02,\n",
              "                        5.0625e-02,  6.6921e-02, -2.6557e-02,  4.5165e-02, -6.2437e-02,\n",
              "                        5.9032e-02, -6.5928e-02,  8.4149e-02,  6.0290e-02, -5.0669e-02,\n",
              "                       -8.5936e-02, -1.5016e-02, -5.3493e-02, -2.8259e-02, -6.3589e-02,\n",
              "                       -3.6025e-02,  7.0225e-02, -1.0557e-01,  2.2705e-02, -2.5558e-02,\n",
              "                       -3.7557e-02, -2.8882e-02, -7.3950e-02,  4.8932e-02,  2.9757e-02,\n",
              "                        1.0250e-01, -7.8808e-02,  9.3256e-02, -7.7098e-02,  1.0485e-01,\n",
              "                       -1.1361e-01,  5.5366e-02, -1.0340e-01,  1.0793e-01, -8.5142e-02,\n",
              "                       -1.1918e-01, -1.1430e-01,  2.5970e-02, -7.1411e-02, -1.9233e-02,\n",
              "                       -2.9050e-02,  6.7337e-02,  1.2094e-01,  5.8936e-02, -8.3704e-02,\n",
              "                       -5.9608e-02, -3.7781e-02, -3.8833e-02,  5.7654e-02, -8.9860e-02,\n",
              "                       -1.0037e-01, -3.5122e-02,  3.0287e-02, -8.9915e-02,  3.3403e-02,\n",
              "                        9.7891e-02, -8.3127e-02,  1.6211e-02,  5.9710e-02, -9.2784e-02,\n",
              "                        2.4261e-02, -1.2660e-01, -6.8822e-02, -6.1236e-02,  2.0379e-02,\n",
              "                       -1.0681e-01, -1.0025e-01,  9.5824e-02,  4.7422e-03,  4.5181e-02,\n",
              "                       -1.2985e-02,  9.4029e-02,  2.0877e-02, -1.0321e-01,  4.9123e-02,\n",
              "                       -4.0894e-02,  4.0809e-02,  1.2164e-01,  7.6397e-02, -1.1965e-01,\n",
              "                        8.0420e-02, -8.7519e-02, -1.1045e-01,  1.1609e-01,  1.1443e-02,\n",
              "                       -9.8265e-02, -2.4068e-02,  7.5193e-02,  1.0973e-01,  9.9945e-02,\n",
              "                        1.0059e-01, -5.7536e-02,  7.6402e-02, -6.6995e-03,  1.1567e-02,\n",
              "                       -7.6850e-02,  1.3113e-01,  9.1638e-02,  8.7768e-02,  2.1057e-02,\n",
              "                       -8.2443e-02, -1.3426e-02, -8.2702e-02, -9.3669e-02,  9.8450e-02,\n",
              "                       -8.9422e-02,  5.5492e-02, -7.3409e-02, -1.2237e-01,  9.0133e-02,\n",
              "                        8.6214e-02,  1.1411e-01,  1.1969e-01,  9.3360e-02,  2.7268e-02,\n",
              "                       -3.8216e-02,  3.6223e-02,  6.8148e-02, -7.1391e-02,  3.5727e-03,\n",
              "                        3.8893e-02, -9.0483e-02, -6.6390e-02,  2.5167e-02,  1.0859e-01,\n",
              "                       -6.8855e-02, -1.0391e-01, -8.2300e-02,  4.9009e-02,  1.1830e-01,\n",
              "                        3.2800e-02,  9.8025e-02, -1.6431e-02, -7.7964e-02,  4.9809e-02,\n",
              "                       -8.3133e-02,  2.2508e-02, -8.0074e-02, -3.9977e-02, -3.7447e-02,\n",
              "                       -1.0490e-01,  1.3374e-01,  4.3683e-02, -1.2539e-01,  1.0385e-01,\n",
              "                        1.2102e-01,  4.3846e-02, -7.8083e-02,  8.8848e-02,  1.1737e-01,\n",
              "                        4.7945e-02, -1.2054e-01,  2.7006e-02,  1.3002e-01, -1.6060e-02,\n",
              "                       -6.2366e-02, -4.8839e-02, -5.9949e-02, -2.6958e-02,  1.2412e-02,\n",
              "                       -7.7108e-02,  7.7790e-02,  4.8713e-02,  7.4588e-02, -1.5640e-02,\n",
              "                        8.7019e-02,  3.1642e-02,  7.0281e-02,  2.7562e-02,  7.6213e-02,\n",
              "                        1.0570e-01,  4.4450e-02,  5.7768e-02,  6.2153e-02, -6.2068e-02,\n",
              "                       -3.5872e-02,  1.7734e-02,  9.1071e-02, -4.4150e-02,  5.7523e-03,\n",
              "                        1.0316e-01,  9.2518e-02,  3.9412e-02, -4.9795e-03,  7.0597e-02,\n",
              "                        1.1168e-01,  1.0101e-01,  5.0818e-02, -4.9160e-02,  4.2363e-02,\n",
              "                        7.7866e-03,  1.2259e-01,  2.1693e-02, -9.0653e-02,  1.7538e-02,\n",
              "                        7.0847e-02, -7.4603e-02, -1.0087e-01, -7.8692e-02, -1.3654e-02,\n",
              "                       -4.6685e-02,  5.9762e-02, -2.3129e-02,  8.4767e-03,  5.8995e-02,\n",
              "                       -1.0492e-01,  5.7312e-03,  3.9879e-02,  8.3697e-03, -1.8815e-02,\n",
              "                       -5.1246e-02, -8.6330e-02, -3.8876e-02, -8.7268e-02, -7.8455e-02,\n",
              "                        1.5770e-03, -7.9123e-02,  6.8305e-02, -1.1216e-01,  1.3960e-01,\n",
              "                       -9.9764e-02, -1.0409e-01, -1.2943e-01,  5.9979e-02,  4.7283e-02,\n",
              "                       -4.9710e-02,  1.0116e-01,  3.1309e-02,  8.3981e-02,  5.4100e-02,\n",
              "                        9.9185e-02,  4.0080e-02, -3.6782e-02,  8.1215e-02,  8.7740e-02,\n",
              "                        8.1753e-02, -9.3779e-02, -6.0230e-02, -5.7857e-02,  8.1985e-02,\n",
              "                       -4.7603e-02, -6.0754e-02, -1.1811e-01, -1.2056e-01,  5.3720e-02,\n",
              "                        8.1738e-02, -3.5592e-02,  8.3765e-02, -5.2369e-02,  1.9524e-02,\n",
              "                        3.0221e-02, -6.9794e-02, -8.3931e-02,  4.9308e-02, -1.1007e-01,\n",
              "                        9.8662e-02,  1.2803e-01, -7.5322e-02, -2.0308e-02, -6.2686e-02,\n",
              "                        1.0359e-01,  3.8981e-02, -7.8743e-02, -4.1031e-02,  7.5089e-02,\n",
              "                        3.4976e-02,  7.0912e-02,  7.1124e-02, -5.2116e-02,  1.2733e-01,\n",
              "                        1.0379e-01,  1.0457e-01,  4.1061e-02,  7.5488e-02, -9.1523e-02,\n",
              "                        1.0894e-01, -5.7767e-02,  7.8992e-02,  1.0637e-01,  1.2610e-01,\n",
              "                       -3.6397e-02, -6.9111e-02, -1.2246e-01,  5.1913e-02,  2.2213e-02,\n",
              "                       -1.1987e-01, -3.1508e-02,  8.8400e-02, -4.4829e-02, -9.4625e-02,\n",
              "                        7.8907e-02, -1.1437e-01, -1.0234e-02,  4.1425e-02,  5.8321e-02,\n",
              "                       -8.7858e-02,  7.8390e-03, -8.6670e-02,  8.2351e-02,  7.8998e-02,\n",
              "                        3.9378e-02,  1.1607e-01,  1.0744e-01, -6.0465e-02, -6.2392e-02,\n",
              "                       -1.0715e-01,  7.3997e-02,  5.5460e-02, -2.6379e-02,  1.0002e-01,\n",
              "                        1.1027e-01,  2.4766e-02, -7.5961e-02,  3.8644e-02, -8.5998e-02,\n",
              "                        5.7795e-02, -9.7195e-02, -9.7587e-02,  5.3818e-02,  5.7053e-02,\n",
              "                       -9.1352e-02,  6.1644e-02,  7.0260e-02,  2.3712e-02,  7.0282e-02,\n",
              "                        3.4023e-02,  9.0136e-02, -1.1929e-01, -1.0761e-01, -7.1590e-02,\n",
              "                       -4.7666e-02, -7.0005e-02,  1.1789e-01, -7.8378e-02, -8.0953e-02,\n",
              "                       -2.9272e-02,  1.1886e-01,  4.8517e-02,  2.0014e-02, -1.9837e-02,\n",
              "                       -1.1877e-01, -5.4324e-02,  1.0596e-01,  6.3951e-03, -1.1854e-01,\n",
              "                       -2.2510e-02,  4.1275e-02,  1.1527e-01, -1.0422e-01,  1.3295e-01,\n",
              "                       -9.4816e-02,  7.4068e-02, -1.6723e-02,  5.8374e-02, -4.2799e-02,\n",
              "                        8.6640e-02,  1.1199e-01,  1.1882e-01, -8.7597e-02, -3.5620e-02,\n",
              "                        8.3502e-02, -1.1390e-01,  2.6556e-02, -8.3470e-02, -6.2794e-02,\n",
              "                        8.2905e-02, -1.0819e-01,  8.8867e-02,  2.7683e-02,  8.8665e-02,\n",
              "                        8.4792e-02, -4.6533e-02, -2.8397e-02,  2.1298e-02,  6.5225e-02,\n",
              "                        3.7069e-02, -6.0554e-02,  3.3175e-02,  1.1003e-01, -6.2359e-02,\n",
              "                       -4.3956e-02,  6.5608e-02,  9.5073e-02, -7.7243e-02,  7.9091e-02]])),\n",
              "             ('fc3.bias', tensor([-0.0320,  0.0010]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfseKPHg0hOG"
      },
      "source": [
        "torch.save(model.state_dict(),'text_classifier_pytorch')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ4iBouH0pkb",
        "outputId": "afce4af7-8dc0-47a3-b50a-09f8d0b4b580"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  text_classifier_pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwKFztcGHdRN"
      },
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "leF7ttuWIBGh",
        "outputId": "654bc5f6-3fba-4937-ee11-7794b399289e"
      },
      "source": [
        "files.download('text_classifier_pytorch')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c943abdd-7058-45b6-a809-c8d265ec1698\", \"text_classifier_pytorch\", 1942979)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5i97IE-35t3"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMvo1EXgILg1"
      },
      "source": [
        "with open('tfidfmodel.pickle','wb') as file:\n",
        "    pickle.dump(vectorizer,file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "iCoZZiLY32Cf",
        "outputId": "7ff4f9c3-dd88-4ca9-f2b0-04d89d2c1ef1"
      },
      "source": [
        "files.download('tfidfmodel.pickle')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a8c8eb19-8436-4c10-849b-53ad20da306a\", \"tfidfmodel.pickle\", 49660)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfBgQDtc39yV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}